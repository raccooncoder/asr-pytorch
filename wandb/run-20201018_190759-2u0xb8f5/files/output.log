cuda:0
/home/raccooncoder/.venv/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
  
Train Epoch: 0 [3840/11775 (33%)]	Loss: 7.279987
Train Epoch: 0 [7680/11775 (65%)]	Loss: 2.881953
Train Epoch: 0 [11520/11775 (98%)]	Loss: 2.748039

Train Epoch: 1 [3840/11775 (33%)]	Loss: 2.730890
Train Epoch: 1 [7680/11775 (65%)]	Loss: 2.727179
Train Epoch: 1 [11520/11775 (98%)]	Loss: 2.721985

Train Epoch: 2 [3840/11775 (33%)]	Loss: 2.719972
Train Epoch: 2 [7680/11775 (65%)]	Loss: 2.714920
Train Epoch: 2 [11520/11775 (98%)]	Loss: 2.711261

Train Epoch: 3 [3840/11775 (33%)]	Loss: 2.699904
Train Epoch: 3 [7680/11775 (65%)]	Loss: 2.692032
Train Epoch: 3 [11520/11775 (98%)]	Loss: 2.679277

Train Epoch: 4 [3840/11775 (33%)]	Loss: 2.673575
Train Epoch: 4 [7680/11775 (65%)]	Loss: 2.654516
Train Epoch: 4 [11520/11775 (98%)]	Loss: 2.638411

Train Epoch: 5 [3840/11775 (33%)]	Loss: 2.608301
Train Epoch: 5 [7680/11775 (65%)]	Loss: 2.574725
Train Epoch: 5 [11520/11775 (98%)]	Loss: 2.533467

Train Epoch: 6 [3840/11775 (33%)]	Loss: 2.479065
Train Epoch: 6 [7680/11775 (65%)]	Loss: 2.444709
Train Epoch: 6 [11520/11775 (98%)]	Loss: 2.396606

Train Epoch: 7 [3840/11775 (33%)]	Loss: 2.347056
Train Epoch: 7 [7680/11775 (65%)]	Loss: 2.296077
Train Epoch: 7 [11520/11775 (98%)]	Loss: 2.252860

Train Epoch: 8 [3840/11775 (33%)]	Loss: 2.203330
Train Epoch: 8 [7680/11775 (65%)]	Loss: 2.155847
Train Epoch: 8 [11520/11775 (98%)]	Loss: 2.117412

Train Epoch: 9 [3840/11775 (33%)]	Loss: 2.076907
Train Epoch: 9 [7680/11775 (65%)]	Loss: 2.031156
Train Epoch: 9 [11520/11775 (98%)]	Loss: 1.991505

/home/raccooncoder/.venv/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:38: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`

Test set: Average loss: 1.9747, Average CER: 0.824739 Average WER: 0.7380

Train Epoch: 10 [3840/11775 (33%)]	Loss: 1.948552
Train Epoch: 10 [7680/11775 (65%)]	Loss: 1.930843
Train Epoch: 10 [11520/11775 (98%)]	Loss: 1.910735

Train Epoch: 11 [3840/11775 (33%)]	Loss: 1.888511
Train Epoch: 11 [7680/11775 (65%)]	Loss: 1.871050
Train Epoch: 11 [11520/11775 (98%)]	Loss: 1.851802

Train Epoch: 12 [3840/11775 (33%)]	Loss: 1.838355
Train Epoch: 12 [7680/11775 (65%)]	Loss: 1.812885
Train Epoch: 12 [11520/11775 (98%)]	Loss: 1.796483

Train Epoch: 13 [3840/11775 (33%)]	Loss: 1.771193
Train Epoch: 13 [7680/11775 (65%)]	Loss: 1.754334
Train Epoch: 13 [11520/11775 (98%)]	Loss: 1.750456

Train Epoch: 14 [3840/11775 (33%)]	Loss: 1.729603
Train Epoch: 14 [7680/11775 (65%)]	Loss: 1.708815
Train Epoch: 14 [11520/11775 (98%)]	Loss: 1.686597

Train Epoch: 15 [3840/11775 (33%)]	Loss: 1.678603
Train Epoch: 15 [7680/11775 (65%)]	Loss: 1.658120
Train Epoch: 15 [11520/11775 (98%)]	Loss: 1.646977

Train Epoch: 16 [3840/11775 (33%)]	Loss: 1.621098
Train Epoch: 16 [7680/11775 (65%)]	Loss: 1.620755
Train Epoch: 16 [11520/11775 (98%)]	Loss: 1.604725

Train Epoch: 17 [3840/11775 (33%)]	Loss: 1.582866
Train Epoch: 17 [7680/11775 (65%)]	Loss: 1.574539
Train Epoch: 17 [11520/11775 (98%)]	Loss: 1.552518

Train Epoch: 18 [3840/11775 (33%)]	Loss: 1.537468
Train Epoch: 18 [7680/11775 (65%)]	Loss: 1.537500
Train Epoch: 18 [11520/11775 (98%)]	Loss: 1.523817

Train Epoch: 19 [3840/11775 (33%)]	Loss: 1.505957
Train Epoch: 19 [7680/11775 (65%)]	Loss: 1.506693
Train Epoch: 19 [11520/11775 (98%)]	Loss: 1.484427


Test set: Average loss: 1.4925, Average CER: 0.497140 Average WER: 0.5030

Train Epoch: 20 [3840/11775 (33%)]	Loss: 1.473685
Train Epoch: 20 [7680/11775 (65%)]	Loss: 1.451014
Train Epoch: 20 [11520/11775 (98%)]	Loss: 1.450801

Train Epoch: 21 [3840/11775 (33%)]	Loss: 1.436350
Train Epoch: 21 [7680/11775 (65%)]	Loss: 1.444305
Train Epoch: 21 [11520/11775 (98%)]	Loss: 1.439635

Train Epoch: 22 [3840/11775 (33%)]	Loss: 1.430864
Train Epoch: 22 [7680/11775 (65%)]	Loss: 1.429991
Train Epoch: 22 [11520/11775 (98%)]	Loss: 1.419819

Train Epoch: 23 [3840/11775 (33%)]	Loss: 1.418380

